# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10e1BEBc4fjvlXsP0YPxLMNo2yCdEkwxK
"""

pip install text2emotion

pip uninstall emoji

pip install emoji==1.7

import text2emotion as te

"""# **Importing Required Libraries**"""

import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import csv
import seaborn as sns

nltk.download('omw-1.4')

"""# **Text2emotion analysis on stories of PREMCHAND**"""

file1 = ""
with open("/content/drive/MyDrive/premchand.csv", "r") as file:
    reader = csv.reader(file)
    for line in file:
      file1 += line
df=pd.read_csv("/content/drive/MyDrive/premchand.csv")
print(te.get_emotion(file1))

"""# **Text2emotion** **analysis** **on** **stories** **of** **Rabindranath** **Tagore**

"""

file1 = ""
with open("/content/drive/MyDrive/Rabindranath-Tagore.csv", "r") as file:
    reader = csv.reader(file)
    for line in file:
      file1 += line
df=pd.read_csv("/content/drive/MyDrive/Rabindranath-Tagore.csv")
print(te.get_emotion(file1))

"""# **Text2emotion analysis on stories of Ruskin Bond**


"""

file1 = ""
with open("/content/drive/MyDrive/ruskinbond.csv", "r") as file:
    reader = csv.reader(file)
    for line in file:
      file1 += line
df=pd.read_csv("/content/drive/MyDrive/ruskinbond.csv")
print(te.get_emotion(file1))

"""# **Text2emotion analysis on stories of RK Narayan**

"""

file1 = ""
with open("/content/drive/MyDrive/RK-Narayan.csv", "r") as file:
    reader = csv.reader(file)
    for line in file:
      file1 += line
df=pd.read_csv("/content/drive/MyDrive/RK-Narayan.csv")
print(te.get_emotion(file1))

"""# **Text2emotion analysis on stories of MulkRaj Anand**

"""

file1 = ""
with open("/content/drive/MyDrive/MulkRaj-Anand.csv", "r") as file:
    reader = csv.reader(file)
    for line in file:
      file1 += line
df=pd.read_csv("/content/drive/MyDrive/MulkRaj-Anand.csv")
print(te.get_emotion(file1))

import cv2
from google.colab.patches import cv2_imshow
img = cv2.imread("/content/drive/MyDrive/WhatsApp Image 2023-02-22 at 11.24.54.jpg")
cv2_imshow(img)
cv2.waitKey(0)

"""# **Splitting** **The** **Mulkraj**-**Anand**.**csv** **File** **Into** **Sentences** """

import csv
import nltk
nltk.download('punkt')

with open('/content/drive/MyDrive/MulkRaj-Anand.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
sentences = []
for text in text_column:
    sentences.extend(nltk.sent_tokenize(text))
results = []
for sentence in sentences: 
    emotions = te.get_emotion(sentence)
    results.append(emotions)
df = pd.DataFrame({'sentence': sentences, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/Mulk-Raj-col.csv', index=True)

"""# **Splitting The Mulkraj-Anand.csv File Into Paragraph**

"""

import csv
with open('/content/drive/MyDrive/MulkRaj-Anand.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []
for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = []
    else:
        current_paragraph.append(text)
if len(current_paragraph) > 0:
    paragraphs.append(" ".join(current_paragraph))
results = []
for paragraph in paragraphs:
    emotions = te.get_emotion(paragraph)
    results.append(emotions)
df = pd.DataFrame({'paragraph': paragraphs, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/Mulk-Raj-par.csv', index=True)

"""# **Splitting** **The** **Rabindranath-Tagore**.**csv** **File** **Into** **Sentences** """

with open('/content/drive/MyDrive/Rabindranath-Tagore.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
sentences = []
for text in text_column:
    sentences.extend(nltk.sent_tokenize(text))
results = []
for sentence in sentences:
    emotions = te.get_emotion(sentence)
    results.append(emotions)
df = pd.DataFrame({'sentence': sentences, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/Rabindranath-Tagore-sen.csv', index=True)

"""# **Splitting** **The** **Rabindranath-Tagore**.**csv** **File** **Into** **Paragraph** """

import csv
with open('/content/drive/MyDrive/Rabindranath-Tagore.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []
for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = []
    else:
        current_paragraph.append(text)
if len(current_paragraph) > 0:
    paragraphs.append(" ".join(current_paragraph))
results = []
for paragraph in paragraphs:
    emotions = te.get_emotion(paragraph)
    results.append(emotions)
df = pd.DataFrame({'paragraph': paragraphs, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/Rabindra.csv', index=True)

"""# **Splitting** **The** **Ruskin Bond**.**csv** **File** **Into** **Sentences** """

with open('/content/drive/MyDrive/ruskinbond.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
sentences = []
for text in text_column:
    sentences.extend(nltk.sent_tokenize(text))
results = []
for sentence in sentences:
    emotions = te.get_emotion(sentence)
    results.append(emotions)
df = pd.DataFrame({'sentence': sentences, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/ruskinbond-sen.csv', index=True)

"""# **Splitting** **The** **Ruskin Bond**.**csv** **File** **Into** **Paragraph** """

import csv
with open('/content/drive/MyDrive/ruskinbond.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []
for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = []
    else:
        current_paragraph.append(text)
if len(current_paragraph) > 0:
    paragraphs.append(" ".join(current_paragraph))
results = []
for paragraph in paragraphs:
    emotions = te.get_emotion(paragraph)
    results.append(emotions)
df = pd.DataFrame({'paragraph': paragraphs, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/ruskinbond_par.csv', index=True)

"""# **Splitting** **The** **RK Narayan**.**csv** **File** **Into** **Paragraph** """

import csv
with open('/content/drive/MyDrive/RK-Narayan.csv','r') as f:
    reader = csv.reader(f)
    data = list(reader)
text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []
for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = []
    else:
        current_paragraph.append(text)
if len(current_paragraph) > 0:
    paragraphs.append(" ".join(current_paragraph))
results = []
for paragraph in paragraphs:
    emotions = te.get_emotion(paragraph)
    results.append(emotions)
df = pd.DataFrame({'paragraph': paragraphs, 'emotions':results})
print(df)
df.to_csv('/content/drive/MyDrive/rk-narayan_par.csv', index=True)

!pip install transformers -q

from transformers import pipeline

emotion = pipeline('sentiment-analysis',model='arpanghoshal/EmoRoBERTa')

File=pd.read_csv("/content/drive/MyDrive/Rabindranath-Tagore-sen.csv")

File.shape

File.head()

def get_emotion_label(text):
  return(emotion(text))[0]['label']

File['emotion'] = File['sentence'].apply(get_emotion_label)

File

sns.countplot(data=File, y='emotion').set_title("Emotion Distribution")

File_Ruskin=pd.read_csv("/content/drive/MyDrive/ruskinbond-sen.csv")

File_Ruskin.shape

File_Ruskin.head()

import csv
import re
import pandas as pd

def count_words(paragraph):
    return len(re.findall(r'\w+', paragraph))

input_csv_file = '/content/drive/MyDrive/RK-Narayan.csv'
output_csv_file = '/content/drive/MyDrive/rk-narayan_pars.csv'

with open(input_csv_file, 'r') as f:
    reader = csv.reader(f)
    data = list(reader)

text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []

for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            combined_paragraph = " ".join(current_paragraph)
            if 250 <= count_words(combined_paragraph):
                paragraphs.append(combined_paragraph)
            current_paragraph = []
    else:
        current_paragraph.append(text)

if len(current_paragraph) > 0:
    combined_paragraph = " ".join(current_paragraph)
    if 250 <= count_words(combined_paragraph):
        paragraphs.append(combined_paragraph)

results = []


df = pd.DataFrame({'paragraph': paragraphs,})
print(df)
df.to_csv(output_csv_file, index=True)

"""# **Fine-Grained analysis on RuskinBond-Sentence Dataset**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import BertConfig, pipeline

# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=41)

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/ruskinbond.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]


# Create a list to store the emotion analysis results for each paragraph
results = []

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Perform emotion analysis and get emotion probabilities
    result = nlp(paragraph, return_all_scores=True)
    
    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })

# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Ruskinbond_FineGrainEmotion.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("Ruskinbond_FineGrainEmotion.csv")
print(df_read)

"""# **Fine-Grained analysis on RK_Narayan-Sentence Dataset**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import BertConfig, pipeline

# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=41)

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/RK-Narayan.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]


# Create a list to store the emotion analysis results for each paragraph
results = []

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Perform emotion analysis and get emotion probabilities
    result = nlp(paragraph, return_all_scores=True)
    
    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })

# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("RK-Narayan_FineGrainEmotion.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("RK-Narayan_FineGrainEmotion.csv")
print(df_read)

"""# **Fine-Grained analysis on Rabindranath_tagore-Sentence Dataset**


"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import BertConfig, pipeline

# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=41)

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/Rabindranath-Tagore.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]


# Create a list to store the emotion analysis results for each paragraph
results = []

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Perform emotion analysis and get emotion probabilities
    result = nlp(paragraph, return_all_scores=True)
    
    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })

# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("RabindraNath_FineGrainEmotion.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("RabindraNath_FineGrainEmotion.csv")
print(df_read)

"""# **Fine-Grained analysis on Premchand-Sentence Dataset**

"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import BertConfig, pipeline

# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=41)

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/Premchand-Paragraph.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]


# Create a list to store the emotion analysis results for each paragraph
results = []

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Perform emotion analysis and get emotion probabilities
    result = nlp(paragraph, return_all_scores=True)
    
    
    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })

# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Premchand_FineGrainEmotion.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("Premchand_FineGrainEmotion.csv")
print(df_read)

"""# **Splitting of Premchand story into paragraph**"""

import csv
import re
import pandas as pd

def count_words(paragraph):
    return len(re.findall(r'\w+', paragraph))

input_csv_file = '/content/drive/MyDrive/RkNarayan_s1.csv'
output_csv_file = '/content/drive/MyDrive/RkNarayan_s1_pars.csv'

with open(input_csv_file, 'r') as f:
    reader = csv.reader(f)
    data = list(reader)

text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []

for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            combined_paragraph = " ".join(current_paragraph)
            if 250 <= count_words(combined_paragraph):
                paragraphs.append(combined_paragraph)
            current_paragraph = []
    else:
        current_paragraph.append(text)

if len(current_paragraph) > 0:
    combined_paragraph = " ".join(current_paragraph)
    if 250 <= count_words(combined_paragraph):
        paragraphs.append(combined_paragraph)

results = []

df = pd.DataFrame(paragraphs, columns=None)
print(df)
df.to_csv(output_csv_file, index=False, header=False)

"""# **Twitter-roberta-base-emotion analysis on Premchand-Paragraph Dataset**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline
import torch


# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')
model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/RkNarayan_s1_par.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]

# Create a list to store the emotion analysis results for each paragraph
results = []

def truncate_text(text, max_chars):
    return text[:max_chars].rsplit(' ', 1)[0]

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Truncate the paragraph to a certain number of characters
    truncated_paragraph = truncate_text(paragraph, 400)

    # Perform emotion analysis and get emotion probabilities
    result = nlp(truncated_paragraph, return_all_scores=True)

    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })


# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Premchand_Twitter_roberta.csv", index=False)


# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()



# Create a DataFrame from the probability matrix and emotions
df = pd.DataFrame(prob_matrix, columns=emotions)

# Add a new column for Paragraph Number
df.insert(0, 'Paragraph', [f"Para {i+1}" for i in range(len(results))])

def color_column(data, color):
    """
    Returns color style for data according to the color provided
    """
    return ['background-color: %s' % color for v in data]

# Specify the colors for each column in a dictionary
column_colors = {'anger': 'blue', 'joy': 'orange', 'optimism': 'green', 'sadness': 'red'}  # replace with your column names and colors

# Apply color to each column
styled_df = df.style.apply(lambda data: color_column(data, column_colors[data.name] if data.name in column_colors else 'default'),
                           subset=pd.IndexSlice[:, list(column_colors.keys())])

styled_df



import csv

def count_words_in_csv(file_path):
    word_count = 0

    with open("/content/drive/MyDrive/RkNarayan_s1_par.csv", 'r') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            for word in row:
                # Split the row into words using whitespace as the delimiter
                words = word.split()
                # Count the number of words in each row
                word_count += len(words)

    return word_count

# Example usage:
csv_file_path = 'path/to/your/file.csv'
total_words = count_words_in_csv(csv_file_path)
print("Total number of words in the CSV file:", total_words)

"""# **Splitting of Mulkraj story into paragraph**"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import re
import pandas as pd

def count_words(paragraph):
    return len(re.findall(r'\w+', paragraph))

input_csv_file = '/content/drive/MyDrive/Mulkraj.csv'
output_csv_file = '/content/drive/MyDrive/Mulkraj-Paragraph.csv'

with open(input_csv_file, 'r') as f:
    reader = csv.reader(f)
    data = list(reader)

text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []

for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            combined_paragraph = " ".join(current_paragraph)
            if 250 <= count_words(combined_paragraph):
                paragraphs.append(combined_paragraph)
            current_paragraph = []
    else:
        current_paragraph.append(text)

if len(current_paragraph) > 0:
    combined_paragraph = " ".join(current_paragraph)
    if 250 <= count_words(combined_paragraph):
        paragraphs.append(combined_paragraph)

results = []


df = pd.DataFrame({'paragraph': paragraphs,})
print(df)
df.to_csv(output_csv_file, index=True)

"""# **Twitter-roberta-base-emotion analysis on MulkRaj-Paragraph Dataset**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline
import torch


# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')
model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/Mulkraj-Paragraph.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]

# Create a list to store the emotion analysis results for each paragraph
results = []

def truncate_text(text, max_chars):
    return text[:max_chars].rsplit(' ', 1)[0]

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Truncate the paragraph to a certain number of characters
    truncated_paragraph = truncate_text(paragraph, 400)

    # Perform emotion analysis and get emotion probabilities
    result = nlp(truncated_paragraph, return_all_scores=True)

    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })


# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Mulkraj_Twitter_roberta.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("Mulkraj_Twitter_roberta.csv")
print(df_read)

"""# **Creating a heatmap to visualize the distribution of emotions by paragraph**"""

import pandas as pd
import seaborn as sns

# Read in the CSV file with sentiment and emotion scores
df = pd.read_csv('/content/Mulkraj_Twitter_roberta.csv')

# Convert the 'emotion_probs' column into separate columns for each emotion
emotion_df = df['emotion_probs'].apply(lambda x: dict(eval(x))).apply(pd.Series)
emotion_df['paragraph_num'] = df['paragraph_num']

# Create a pivot table with emotions as columns, sentences as rows, and mean scores as values
pivot_table = emotion_df.pivot_table(index='paragraph_num', aggfunc='mean')

# Create a heatmap to visualize the distribution of emotions by paragraph
sns.heatmap(pivot_table, cmap='coolwarm')

"""# **Splitting of Ruskinbond story into paragraph**"""

import csv
import re
import pandas as pd

def count_words(paragraph):
    return len(re.findall(r'\w+', paragraph))

input_csv_file = '/content/drive/MyDrive/RuskinBond.csv'
output_csv_file = '/content/drive/MyDrive/Ruskinbond-Paragraph.csv'

with open(input_csv_file, 'r') as f:
    reader = csv.reader(f)
    data = list(reader)

text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []

for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            combined_paragraph = " ".join(current_paragraph)
            if 250 <= count_words(combined_paragraph):
                paragraphs.append(combined_paragraph)
            current_paragraph = []
    else:
        current_paragraph.append(text)

if len(current_paragraph) > 0:
    combined_paragraph = " ".join(current_paragraph)
    if 250 <= count_words(combined_paragraph):
        paragraphs.append(combined_paragraph)

results = []


df = pd.DataFrame({'paragraph': paragraphs,})
print(df)
df.to_csv(output_csv_file, index=True)

"""# **Twitter-roberta-base-emotion analysis on RuskinBond-Paragraph Dataset**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline
import torch


# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')
model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/Ruskinbond-paragraph.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]

# Create a list to store the emotion analysis results for each paragraph
results = []

def truncate_text(text, max_chars):
    return text[:max_chars].rsplit(' ', 1)[0]

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Truncate the paragraph to a certain number of characters
    truncated_paragraph = truncate_text(paragraph, 400)

    # Perform emotion analysis and get emotion probabilities
    result = nlp(truncated_paragraph, return_all_scores=True)

    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })


# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Ruskinbond_Twitter_roberta.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("Ruskinbond_Twitter_roberta.csv")
print(df_read)

"""# **Creating a heatmap to visualize the distribution of emotions by paragraph**"""

import pandas as pd
import seaborn as sns
df = pd.read_csv('/content/Ruskinbond_Twitter_roberta.csv')
emotion_df = df['emotion_probs'].apply(lambda x: dict(eval(x))).apply(pd.Series)
emotion_df['paragraph_num'] = df['paragraph_num']
pivot_table = emotion_df.pivot_table(index='paragraph_num', aggfunc='mean')
sns.heatmap(pivot_table, cmap='coolwarm')

"""# **Splitting of Premchand story into paragraph**


"""

import csv
import re
import pandas as pd

def count_words(paragraph):
    return len(re.findall(r'\w+', paragraph))

input_csv_file = '/content/drive/MyDrive/Premchand.csv'
output_csv_file = '/content/drive/MyDrive/Premchand-Paragraph.csv'

with open(input_csv_file, 'r') as f:
    reader = csv.reader(f)
    data = list(reader)

text_column = [row[0] for row in data]
paragraphs = []
current_paragraph = []

for text in text_column:
    if text.strip() == "":
        if len(current_paragraph) > 0:
            combined_paragraph = " ".join(current_paragraph)
            if 250 <= count_words(combined_paragraph):
                paragraphs.append(combined_paragraph)
            current_paragraph = []
    else:
        current_paragraph.append(text)

if len(current_paragraph) > 0:
    combined_paragraph = " ".join(current_paragraph)
    if 250 <= count_words(combined_paragraph):
        paragraphs.append(combined_paragraph)

results = []


df = pd.DataFrame({'paragraph': paragraphs,})
print(df)
df.to_csv(output_csv_file, index=True)

"""# **Twitter-roberta-base-emotion analysis on Rabindranath-Paragraph Datase**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline
import torch


# Load the tokenizer and model for fine-grained emotion analysis
tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')
model = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')

# Load the emotion analysis pipeline
nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Read the input CSV file without headers
input_df = pd.read_csv("/content/drive/MyDrive/Rabindranath-paragraph.csv", header=None)

# Get the paragraphs from the input DataFrame and filter out non-string items
paragraphs = [p for p in input_df[0].tolist() if isinstance(p, str)]

# Create a list to store the emotion analysis results for each paragraph
results = []

def truncate_text(text, max_chars):
    return text[:max_chars].rsplit(' ', 1)[0]

# Loop through each paragraph, perform emotion analysis, and store the results
for i, paragraph in enumerate(paragraphs):
    # Truncate the paragraph to a certain number of characters
    truncated_paragraph = truncate_text(paragraph, 400)

    # Perform emotion analysis and get emotion probabilities
    result = nlp(truncated_paragraph, return_all_scores=True)

    # Extract emotion probabilities
    emotion_probs = []
    for entry in result:
        for dicts in entry:
            emotion_probs.append((dicts['label'], round(dicts['score'], 3)))

    # Store the results as a dictionary
    results.append({
        "paragraph_num": i+1,
        "paragraph_text": paragraph,
        "emotion_probs": emotion_probs
    })


# Convert the results to a pandas DataFrame and save as a CSV file
df = pd.DataFrame(results)
df.to_csv("Rabindranath_Twitter_roberta.csv", index=False)

# Create a stacked bar chart to visualize the evolution of emotions
emotions = sorted(list(set([label for result in results for label, _ in result["emotion_probs"]])))

prob_matrix = np.zeros((len(results), len(emotions)))
for i, result in enumerate(results):
    for j, emotion in enumerate(emotions):
        for label, prob in result["emotion_probs"]:
            if label == emotion:
                prob_matrix[i, j] = prob
fig, ax = plt.subplots()
ax.bar(range(len(results)), prob_matrix[:, 0], label=emotions[0])
for i in range(1, len(emotions)):
    ax.bar(range(len(results)), prob_matrix[:, i], bottom=prob_matrix[:, i-1], label=emotions[i])

ax.set_xticks(range(len(results)))
ax.set_xticklabels([f"Para {i+1}" for i in range(len(results))])
ax.legend()
ax.set_ylabel("Probability")
ax.set_title("Evolution of Emotions")
plt.show()

# Read output.csv file
df_read = pd.read_csv("Rabindranath_Twitter_roberta.csv")
print(df_read)

"""# **Creating a heatmap to visualize the distribution of emotions by paragraph**"""

import pandas as pd
import seaborn as sns

# Read in the CSV file with sentiment and emotion scores
df = pd.read_csv('/content/Rabindranath_Twitter_roberta.csv')

# Convert the 'emotion_probs' column into separate columns for each emotion
emotion_df = df['emotion_probs'].apply(lambda x: dict(eval(x))).apply(pd.Series)
emotion_df['paragraph_num'] = df['paragraph_num']

# Create a pivot table with emotions as columns, sentences as rows, and mean scores as values
pivot_table = emotion_df.pivot_table(index='paragraph_num', aggfunc='mean')

# Create a heatmap to visualize the distribution of emotions by paragraph
sns.heatmap(pivot_table, cmap='coolwarm')